{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "750/750 [==============================] - 11s 14ms/step - loss: 0.5821 - accuracy: 0.7855 - val_loss: 0.4261 - val_accuracy: 0.8407\n",
      "Epoch 2/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.3681 - accuracy: 0.8655 - val_loss: 0.3330 - val_accuracy: 0.8777\n",
      "Epoch 3/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.3118 - accuracy: 0.8859 - val_loss: 0.3230 - val_accuracy: 0.8819\n",
      "Epoch 4/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2778 - accuracy: 0.8976 - val_loss: 0.3072 - val_accuracy: 0.8881\n",
      "Epoch 5/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2571 - accuracy: 0.9043 - val_loss: 0.2756 - val_accuracy: 0.8981\n",
      "Epoch 6/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2342 - accuracy: 0.9123 - val_loss: 0.2677 - val_accuracy: 0.9028\n",
      "Epoch 7/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2158 - accuracy: 0.9191 - val_loss: 0.2634 - val_accuracy: 0.9036\n",
      "Epoch 8/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2015 - accuracy: 0.9244 - val_loss: 0.2631 - val_accuracy: 0.9057\n",
      "Epoch 9/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1851 - accuracy: 0.9321 - val_loss: 0.2483 - val_accuracy: 0.9117\n",
      "Epoch 10/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1733 - accuracy: 0.9356 - val_loss: 0.2687 - val_accuracy: 0.9064\n",
      "Epoch 11/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.1574 - accuracy: 0.9414 - val_loss: 0.2586 - val_accuracy: 0.9108\n",
      "Epoch 12/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1448 - accuracy: 0.9468 - val_loss: 0.2739 - val_accuracy: 0.9105\n",
      "Epoch 13/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1335 - accuracy: 0.9497 - val_loss: 0.2781 - val_accuracy: 0.9123\n",
      "Epoch 14/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1253 - accuracy: 0.9526 - val_loss: 0.2945 - val_accuracy: 0.9096\n",
      "Epoch 15/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1133 - accuracy: 0.9566 - val_loss: 0.2990 - val_accuracy: 0.9054\n",
      "Epoch 16/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1063 - accuracy: 0.9593 - val_loss: 0.2994 - val_accuracy: 0.9080\n",
      "Epoch 17/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0970 - accuracy: 0.9627 - val_loss: 0.3148 - val_accuracy: 0.9122\n",
      "Epoch 18/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0892 - accuracy: 0.9656 - val_loss: 0.3247 - val_accuracy: 0.9076\n",
      "Epoch 19/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0828 - accuracy: 0.9680 - val_loss: 0.3469 - val_accuracy: 0.9061\n",
      "Epoch 20/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0802 - accuracy: 0.9701 - val_loss: 0.3468 - val_accuracy: 0.9094\n",
      "Epoch 21/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0677 - accuracy: 0.9751 - val_loss: 0.3765 - val_accuracy: 0.9076\n",
      "Epoch 22/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0664 - accuracy: 0.9744 - val_loss: 0.3734 - val_accuracy: 0.9098\n",
      "Epoch 23/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0627 - accuracy: 0.9760 - val_loss: 0.4004 - val_accuracy: 0.9033\n",
      "Epoch 24/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0573 - accuracy: 0.9783 - val_loss: 0.4010 - val_accuracy: 0.9050\n",
      "Epoch 25/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0568 - accuracy: 0.9790 - val_loss: 0.4267 - val_accuracy: 0.9062\n",
      "Epoch 26/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0542 - accuracy: 0.9792 - val_loss: 0.4332 - val_accuracy: 0.9053\n",
      "Epoch 27/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0448 - accuracy: 0.9833 - val_loss: 0.4602 - val_accuracy: 0.9068\n",
      "Epoch 28/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0493 - accuracy: 0.9818 - val_loss: 0.4547 - val_accuracy: 0.9039\n",
      "Epoch 29/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0447 - accuracy: 0.9832 - val_loss: 0.5189 - val_accuracy: 0.9047\n",
      "Epoch 30/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0425 - accuracy: 0.9841 - val_loss: 0.4868 - val_accuracy: 0.9062\n",
      "Epoch 31/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0369 - accuracy: 0.9864 - val_loss: 0.5222 - val_accuracy: 0.9054\n",
      "Epoch 32/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0384 - accuracy: 0.9859 - val_loss: 0.5800 - val_accuracy: 0.9015\n",
      "Epoch 33/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0424 - accuracy: 0.9843 - val_loss: 0.5065 - val_accuracy: 0.9018\n",
      "Epoch 34/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0322 - accuracy: 0.9883 - val_loss: 0.5480 - val_accuracy: 0.9038\n",
      "Epoch 35/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0300 - accuracy: 0.9890 - val_loss: 0.6062 - val_accuracy: 0.9028\n",
      "Epoch 36/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0445 - accuracy: 0.9839 - val_loss: 0.5497 - val_accuracy: 0.9029\n",
      "Epoch 37/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0268 - accuracy: 0.9899 - val_loss: 0.5971 - val_accuracy: 0.9069\n",
      "Epoch 38/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0339 - accuracy: 0.9877 - val_loss: 0.6190 - val_accuracy: 0.9050\n",
      "Epoch 39/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0342 - accuracy: 0.9875 - val_loss: 0.5860 - val_accuracy: 0.9024\n",
      "Epoch 40/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0352 - accuracy: 0.9873 - val_loss: 0.5942 - val_accuracy: 0.9036\n",
      "Epoch 41/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0252 - accuracy: 0.9908 - val_loss: 0.6382 - val_accuracy: 0.9043\n",
      "Epoch 42/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0294 - accuracy: 0.9889 - val_loss: 0.6275 - val_accuracy: 0.9011\n",
      "Epoch 43/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0306 - accuracy: 0.9892 - val_loss: 0.6628 - val_accuracy: 0.9082\n",
      "Epoch 44/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0210 - accuracy: 0.9923 - val_loss: 0.6956 - val_accuracy: 0.9047\n",
      "Epoch 45/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0351 - accuracy: 0.9875 - val_loss: 0.6554 - val_accuracy: 0.9092\n",
      "Epoch 46/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0334 - accuracy: 0.9884 - val_loss: 0.6724 - val_accuracy: 0.9074\n",
      "Epoch 47/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0205 - accuracy: 0.9928 - val_loss: 0.6921 - val_accuracy: 0.9051\n",
      "Epoch 48/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0227 - accuracy: 0.9921 - val_loss: 0.7106 - val_accuracy: 0.8938\n",
      "Epoch 49/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0305 - accuracy: 0.9895 - val_loss: 0.6655 - val_accuracy: 0.9016\n",
      "Epoch 50/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0183 - accuracy: 0.9943 - val_loss: 0.7706 - val_accuracy: 0.9038\n",
      "Epoch 51/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0255 - accuracy: 0.9909 - val_loss: 0.7448 - val_accuracy: 0.9053\n",
      "Epoch 52/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0326 - accuracy: 0.9888 - val_loss: 0.7600 - val_accuracy: 0.9045\n",
      "Epoch 53/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0181 - accuracy: 0.9934 - val_loss: 0.7659 - val_accuracy: 0.9043\n",
      "Epoch 54/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0286 - accuracy: 0.9898 - val_loss: 0.7425 - val_accuracy: 0.9065\n",
      "Epoch 55/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0244 - accuracy: 0.9915 - val_loss: 0.7850 - val_accuracy: 0.9036\n",
      "Epoch 56/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0227 - accuracy: 0.9921 - val_loss: 0.7430 - val_accuracy: 0.9023\n",
      "Epoch 57/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0272 - accuracy: 0.9902 - val_loss: 0.7845 - val_accuracy: 0.9013\n",
      "Epoch 58/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0179 - accuracy: 0.9938 - val_loss: 0.8203 - val_accuracy: 0.9007\n",
      "Epoch 59/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0327 - accuracy: 0.9885 - val_loss: 0.7621 - val_accuracy: 0.9047\n",
      "Epoch 60/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0178 - accuracy: 0.9940 - val_loss: 0.8262 - val_accuracy: 0.9062\n",
      "Epoch 61/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0269 - accuracy: 0.9911 - val_loss: 0.8303 - val_accuracy: 0.9035\n",
      "Epoch 62/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0221 - accuracy: 0.9920 - val_loss: 0.7811 - val_accuracy: 0.9046\n",
      "Epoch 63/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 0.8016 - val_accuracy: 0.9012\n",
      "Epoch 64/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0197 - accuracy: 0.9934 - val_loss: 0.8169 - val_accuracy: 0.9040\n",
      "Epoch 65/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0135 - accuracy: 0.9952 - val_loss: 0.8411 - val_accuracy: 0.9038\n",
      "Epoch 66/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0194 - accuracy: 0.9936 - val_loss: 0.9428 - val_accuracy: 0.9067\n",
      "Epoch 67/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.7972 - val_accuracy: 0.9053\n",
      "Epoch 68/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.8598 - val_accuracy: 0.9047\n",
      "Epoch 69/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0235 - accuracy: 0.9917 - val_loss: 0.8592 - val_accuracy: 0.9028\n",
      "Epoch 70/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0236 - accuracy: 0.9924 - val_loss: 0.8118 - val_accuracy: 0.9024\n",
      "Epoch 71/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0177 - accuracy: 0.9940 - val_loss: 0.8747 - val_accuracy: 0.9058\n",
      "Epoch 72/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0221 - accuracy: 0.9929 - val_loss: 0.8872 - val_accuracy: 0.9030\n",
      "Epoch 73/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0182 - accuracy: 0.9941 - val_loss: 0.8521 - val_accuracy: 0.9072\n",
      "Epoch 74/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.8878 - val_accuracy: 0.9082\n",
      "Epoch 75/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.8731 - val_accuracy: 0.9021\n",
      "Epoch 76/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0245 - accuracy: 0.9918 - val_loss: 0.8746 - val_accuracy: 0.9050\n",
      "Epoch 77/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.8698 - val_accuracy: 0.9069\n",
      "Epoch 78/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0207 - accuracy: 0.9934 - val_loss: 0.8266 - val_accuracy: 0.9024\n",
      "Epoch 79/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0190 - accuracy: 0.9939 - val_loss: 0.8789 - val_accuracy: 0.8978\n",
      "Epoch 80/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.9226 - val_accuracy: 0.9038\n",
      "Epoch 81/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0181 - accuracy: 0.9938 - val_loss: 0.8911 - val_accuracy: 0.9069\n",
      "Epoch 82/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.9637 - val_accuracy: 0.9013\n",
      "Epoch 83/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.9753 - val_accuracy: 0.9039\n",
      "Epoch 84/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0149 - accuracy: 0.9952 - val_loss: 0.9332 - val_accuracy: 0.9083\n",
      "Epoch 85/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0172 - accuracy: 0.9940 - val_loss: 0.9478 - val_accuracy: 0.9030\n",
      "Epoch 86/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0179 - accuracy: 0.9936 - val_loss: 0.9827 - val_accuracy: 0.9058\n",
      "Epoch 87/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.9970 - val_accuracy: 0.9065\n",
      "Epoch 88/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0229 - accuracy: 0.9930 - val_loss: 0.9540 - val_accuracy: 0.9018\n",
      "Epoch 89/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0177 - accuracy: 0.9945 - val_loss: 1.0295 - val_accuracy: 0.9072\n",
      "Epoch 90/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0164 - accuracy: 0.9949 - val_loss: 0.9563 - val_accuracy: 0.9040\n",
      "Epoch 91/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0189 - accuracy: 0.9939 - val_loss: 0.9821 - val_accuracy: 0.9064\n",
      "Epoch 92/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0190 - accuracy: 0.9944 - val_loss: 0.9860 - val_accuracy: 0.8999\n",
      "Epoch 93/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0182 - accuracy: 0.9935 - val_loss: 0.9836 - val_accuracy: 0.9025\n",
      "Epoch 94/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0130 - accuracy: 0.9958 - val_loss: 0.9536 - val_accuracy: 0.9069\n",
      "Epoch 95/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0226 - accuracy: 0.9929 - val_loss: 0.9805 - val_accuracy: 0.9029\n",
      "Epoch 96/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0125 - accuracy: 0.9961 - val_loss: 1.0429 - val_accuracy: 0.9062\n",
      "Epoch 97/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0182 - accuracy: 0.9943 - val_loss: 1.0435 - val_accuracy: 0.9033\n",
      "Epoch 98/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.9914 - val_accuracy: 0.9061\n",
      "Epoch 99/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0176 - accuracy: 0.9941 - val_loss: 1.0333 - val_accuracy: 0.9043\n",
      "Epoch 100/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0244 - accuracy: 0.9927 - val_loss: 0.9498 - val_accuracy: 0.9025\n",
      "Model trained on original, tested on original Fashion MNIST test accuracy: 90.31%\n",
      "Model trained on original, tested on modified Fashion MNIST test accuracy: 30.21%\n",
      "Epoch 1/100\n",
      "750/750 [==============================] - 11s 13ms/step - loss: 0.6163 - accuracy: 0.7779 - val_loss: 0.4306 - val_accuracy: 0.8443\n",
      "Epoch 2/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.3849 - accuracy: 0.8604 - val_loss: 0.3555 - val_accuracy: 0.8677\n",
      "Epoch 3/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.3348 - accuracy: 0.8773 - val_loss: 0.3426 - val_accuracy: 0.8702\n",
      "Epoch 4/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.3025 - accuracy: 0.8874 - val_loss: 0.3024 - val_accuracy: 0.8896\n",
      "Epoch 5/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2819 - accuracy: 0.8960 - val_loss: 0.2951 - val_accuracy: 0.8910\n",
      "Epoch 6/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2648 - accuracy: 0.9036 - val_loss: 0.2905 - val_accuracy: 0.8923\n",
      "Epoch 7/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2488 - accuracy: 0.9078 - val_loss: 0.2774 - val_accuracy: 0.8979\n",
      "Epoch 8/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2340 - accuracy: 0.9128 - val_loss: 0.2824 - val_accuracy: 0.8935\n",
      "Epoch 9/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2224 - accuracy: 0.9176 - val_loss: 0.2619 - val_accuracy: 0.9046\n",
      "Epoch 10/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2100 - accuracy: 0.9224 - val_loss: 0.2805 - val_accuracy: 0.9003\n",
      "Epoch 11/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.2003 - accuracy: 0.9242 - val_loss: 0.2640 - val_accuracy: 0.9048\n",
      "Epoch 12/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1896 - accuracy: 0.9282 - val_loss: 0.2614 - val_accuracy: 0.9049\n",
      "Epoch 13/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1809 - accuracy: 0.9317 - val_loss: 0.2785 - val_accuracy: 0.8988\n",
      "Epoch 14/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1709 - accuracy: 0.9350 - val_loss: 0.2698 - val_accuracy: 0.9028\n",
      "Epoch 15/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1617 - accuracy: 0.9399 - val_loss: 0.2721 - val_accuracy: 0.9031\n",
      "Epoch 16/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1531 - accuracy: 0.9426 - val_loss: 0.2987 - val_accuracy: 0.8995\n",
      "Epoch 17/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1443 - accuracy: 0.9459 - val_loss: 0.2935 - val_accuracy: 0.9013\n",
      "Epoch 18/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1362 - accuracy: 0.9481 - val_loss: 0.2882 - val_accuracy: 0.9061\n",
      "Epoch 19/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1306 - accuracy: 0.9502 - val_loss: 0.3057 - val_accuracy: 0.8984\n",
      "Epoch 20/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1243 - accuracy: 0.9532 - val_loss: 0.3063 - val_accuracy: 0.9041\n",
      "Epoch 21/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1122 - accuracy: 0.9578 - val_loss: 0.3232 - val_accuracy: 0.9038\n",
      "Epoch 22/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1079 - accuracy: 0.9595 - val_loss: 0.3461 - val_accuracy: 0.9034\n",
      "Epoch 23/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.1025 - accuracy: 0.9615 - val_loss: 0.3586 - val_accuracy: 0.9022\n",
      "Epoch 24/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0990 - accuracy: 0.9618 - val_loss: 0.3533 - val_accuracy: 0.9030\n",
      "Epoch 25/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0934 - accuracy: 0.9640 - val_loss: 0.3694 - val_accuracy: 0.9035\n",
      "Epoch 26/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0853 - accuracy: 0.9675 - val_loss: 0.3805 - val_accuracy: 0.8987\n",
      "Epoch 27/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0812 - accuracy: 0.9697 - val_loss: 0.4004 - val_accuracy: 0.8975\n",
      "Epoch 28/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0808 - accuracy: 0.9690 - val_loss: 0.4195 - val_accuracy: 0.8968\n",
      "Epoch 29/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0701 - accuracy: 0.9735 - val_loss: 0.4054 - val_accuracy: 0.8988\n",
      "Epoch 30/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0689 - accuracy: 0.9741 - val_loss: 0.4304 - val_accuracy: 0.8981\n",
      "Epoch 31/100\n",
      "750/750 [==============================] - 11s 14ms/step - loss: 0.0676 - accuracy: 0.9741 - val_loss: 0.4169 - val_accuracy: 0.8980\n",
      "Epoch 32/100\n",
      "750/750 [==============================] - 11s 14ms/step - loss: 0.0633 - accuracy: 0.9768 - val_loss: 0.4341 - val_accuracy: 0.8975\n",
      "Epoch 33/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0611 - accuracy: 0.9769 - val_loss: 0.4859 - val_accuracy: 0.8969\n",
      "Epoch 34/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0636 - accuracy: 0.9752 - val_loss: 0.4700 - val_accuracy: 0.9007\n",
      "Epoch 35/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0554 - accuracy: 0.9791 - val_loss: 0.4375 - val_accuracy: 0.9001\n",
      "Epoch 36/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0538 - accuracy: 0.9795 - val_loss: 0.5196 - val_accuracy: 0.8938\n",
      "Epoch 37/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0520 - accuracy: 0.9799 - val_loss: 0.5420 - val_accuracy: 0.8988\n",
      "Epoch 38/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0488 - accuracy: 0.9822 - val_loss: 0.5390 - val_accuracy: 0.8974\n",
      "Epoch 39/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0506 - accuracy: 0.9808 - val_loss: 0.5589 - val_accuracy: 0.8979\n",
      "Epoch 40/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0480 - accuracy: 0.9816 - val_loss: 0.5624 - val_accuracy: 0.8907\n",
      "Epoch 41/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0508 - accuracy: 0.9803 - val_loss: 0.5390 - val_accuracy: 0.8899\n",
      "Epoch 42/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0410 - accuracy: 0.9854 - val_loss: 0.5433 - val_accuracy: 0.8963\n",
      "Epoch 43/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0453 - accuracy: 0.9830 - val_loss: 0.6159 - val_accuracy: 0.8952\n",
      "Epoch 44/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0412 - accuracy: 0.9843 - val_loss: 0.6768 - val_accuracy: 0.8980\n",
      "Epoch 45/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0406 - accuracy: 0.9848 - val_loss: 0.6300 - val_accuracy: 0.8939\n",
      "Epoch 46/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0431 - accuracy: 0.9840 - val_loss: 0.5991 - val_accuracy: 0.8997\n",
      "Epoch 47/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0351 - accuracy: 0.9867 - val_loss: 0.6275 - val_accuracy: 0.8898\n",
      "Epoch 48/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0410 - accuracy: 0.9844 - val_loss: 0.5814 - val_accuracy: 0.8972\n",
      "Epoch 49/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0369 - accuracy: 0.9862 - val_loss: 0.6407 - val_accuracy: 0.8923\n",
      "Epoch 50/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0367 - accuracy: 0.9867 - val_loss: 0.6257 - val_accuracy: 0.8996\n",
      "Epoch 51/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0306 - accuracy: 0.9889 - val_loss: 0.7091 - val_accuracy: 0.8878\n",
      "Epoch 52/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0436 - accuracy: 0.9841 - val_loss: 0.6499 - val_accuracy: 0.8957\n",
      "Epoch 53/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0277 - accuracy: 0.9900 - val_loss: 0.6715 - val_accuracy: 0.9009\n",
      "Epoch 54/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0392 - accuracy: 0.9856 - val_loss: 0.6266 - val_accuracy: 0.8946\n",
      "Epoch 55/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0318 - accuracy: 0.9877 - val_loss: 0.7135 - val_accuracy: 0.8972\n",
      "Epoch 56/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0320 - accuracy: 0.9883 - val_loss: 0.7522 - val_accuracy: 0.8945\n",
      "Epoch 57/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0331 - accuracy: 0.9883 - val_loss: 0.6831 - val_accuracy: 0.8999\n",
      "Epoch 58/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0296 - accuracy: 0.9887 - val_loss: 0.7466 - val_accuracy: 0.8951\n",
      "Epoch 59/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0315 - accuracy: 0.9888 - val_loss: 0.6926 - val_accuracy: 0.8986\n",
      "Epoch 60/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0318 - accuracy: 0.9884 - val_loss: 0.7832 - val_accuracy: 0.8958\n",
      "Epoch 61/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0301 - accuracy: 0.9886 - val_loss: 0.7116 - val_accuracy: 0.8928\n",
      "Epoch 62/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0351 - accuracy: 0.9878 - val_loss: 0.7288 - val_accuracy: 0.8967\n",
      "Epoch 63/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0310 - accuracy: 0.9888 - val_loss: 0.7307 - val_accuracy: 0.8981\n",
      "Epoch 64/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0215 - accuracy: 0.9924 - val_loss: 0.7898 - val_accuracy: 0.8993\n",
      "Epoch 65/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0281 - accuracy: 0.9901 - val_loss: 0.7617 - val_accuracy: 0.8954\n",
      "Epoch 66/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0314 - accuracy: 0.9887 - val_loss: 0.7448 - val_accuracy: 0.8947\n",
      "Epoch 67/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0296 - accuracy: 0.9897 - val_loss: 0.7964 - val_accuracy: 0.8958\n",
      "Epoch 68/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0288 - accuracy: 0.9898 - val_loss: 0.7830 - val_accuracy: 0.9018\n",
      "Epoch 69/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0270 - accuracy: 0.9902 - val_loss: 0.7601 - val_accuracy: 0.8976\n",
      "Epoch 70/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0279 - accuracy: 0.9898 - val_loss: 0.8416 - val_accuracy: 0.8926\n",
      "Epoch 71/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 0.8210 - val_accuracy: 0.8948\n",
      "Epoch 72/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0273 - accuracy: 0.9902 - val_loss: 0.7752 - val_accuracy: 0.8957\n",
      "Epoch 73/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0271 - accuracy: 0.9909 - val_loss: 0.7714 - val_accuracy: 0.9000\n",
      "Epoch 74/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0232 - accuracy: 0.9924 - val_loss: 0.8744 - val_accuracy: 0.8966\n",
      "Epoch 75/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0305 - accuracy: 0.9894 - val_loss: 0.8518 - val_accuracy: 0.8954\n",
      "Epoch 76/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0252 - accuracy: 0.9909 - val_loss: 0.8859 - val_accuracy: 0.8936\n",
      "Epoch 77/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0235 - accuracy: 0.9917 - val_loss: 0.8895 - val_accuracy: 0.8947\n",
      "Epoch 78/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0253 - accuracy: 0.9911 - val_loss: 0.8601 - val_accuracy: 0.8915\n",
      "Epoch 79/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0261 - accuracy: 0.9910 - val_loss: 0.8103 - val_accuracy: 0.8947\n",
      "Epoch 80/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0181 - accuracy: 0.9933 - val_loss: 0.8809 - val_accuracy: 0.8937\n",
      "Epoch 81/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0268 - accuracy: 0.9904 - val_loss: 0.8778 - val_accuracy: 0.8917\n",
      "Epoch 82/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0286 - accuracy: 0.9906 - val_loss: 0.9038 - val_accuracy: 0.8960\n",
      "Epoch 83/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0216 - accuracy: 0.9925 - val_loss: 0.8730 - val_accuracy: 0.8964\n",
      "Epoch 84/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0261 - accuracy: 0.9909 - val_loss: 0.8535 - val_accuracy: 0.8963\n",
      "Epoch 85/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0190 - accuracy: 0.9936 - val_loss: 0.8940 - val_accuracy: 0.8957\n",
      "Epoch 86/100\n",
      "750/750 [==============================] - 11s 14ms/step - loss: 0.0306 - accuracy: 0.9898 - val_loss: 0.8646 - val_accuracy: 0.8964\n",
      "Epoch 87/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0241 - accuracy: 0.9913 - val_loss: 0.9033 - val_accuracy: 0.9009\n",
      "Epoch 88/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0216 - accuracy: 0.9928 - val_loss: 0.8586 - val_accuracy: 0.8947\n",
      "Epoch 89/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0256 - accuracy: 0.9914 - val_loss: 0.9054 - val_accuracy: 0.8947\n",
      "Epoch 90/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.8730 - val_accuracy: 0.8989\n",
      "Epoch 91/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0147 - accuracy: 0.9946 - val_loss: 0.9471 - val_accuracy: 0.8979\n",
      "Epoch 92/100\n",
      "750/750 [==============================] - 11s 14ms/step - loss: 0.0293 - accuracy: 0.9897 - val_loss: 0.8303 - val_accuracy: 0.8946\n",
      "Epoch 93/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0256 - accuracy: 0.9915 - val_loss: 0.9241 - val_accuracy: 0.8962\n",
      "Epoch 94/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0203 - accuracy: 0.9933 - val_loss: 0.9128 - val_accuracy: 0.8939\n",
      "Epoch 95/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0198 - accuracy: 0.9931 - val_loss: 0.9440 - val_accuracy: 0.8938\n",
      "Epoch 96/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0272 - accuracy: 0.9911 - val_loss: 0.9817 - val_accuracy: 0.8930\n",
      "Epoch 97/100\n",
      "750/750 [==============================] - 10s 13ms/step - loss: 0.0143 - accuracy: 0.9955 - val_loss: 0.9678 - val_accuracy: 0.8978\n",
      "Epoch 98/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0265 - accuracy: 0.9912 - val_loss: 0.9775 - val_accuracy: 0.8966\n",
      "Epoch 99/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 0.9512 - val_accuracy: 0.8932\n",
      "Epoch 100/100\n",
      "750/750 [==============================] - 10s 14ms/step - loss: 0.0227 - accuracy: 0.9925 - val_loss: 0.9588 - val_accuracy: 0.8953\n",
      "Model trained on modified, tested on original Fashion MNIST test accuracy: 25.42%\n",
      "Model trained on modified, tested on modified Fashion MNIST test accuracy: 89.65%\n"
     ]
    }
   ],
   "source": [
    "# Fashion MNIST\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Expand dimensions to add the channel dimension (grayscale images)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, target_size=(28, 28), scale_factor=0.51):\n",
    "    height, width, channels = image.shape[0], image.shape[1], image.shape[2]\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (target_size[0] - new_height) // 2\n",
    "    pad_width = (target_size[1] - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the target size\n",
    "    if channels == 1:\n",
    "        padded_image = np.zeros((target_size[0], target_size[1]), dtype=np.uint8)\n",
    "    else:\n",
    "        padded_image = np.zeros((target_size[0], target_size[1], channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    if channels == 1:\n",
    "        padded_image = np.expand_dims(padded_image, axis=-1)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10  # Fashion MNIST has 10 classes\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Define the model architecture\n",
    "input_shape = (28, 28, 1)  # Input shape for Fashion MNIST images\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the original Fashion MNIST dataset\n",
    "model.fit(x_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original Fashion MNIST dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on original, tested on original Fashion MNIST test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified Fashion MNIST dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on original, tested on modified Fashion MNIST test accuracy: {original_on_modified_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Train a new model on the modified Fashion MNIST dataset\n",
    "model_modified = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model_modified.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_modified.fit(x_train_modified, y_train, epochs=100, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the modified model on the original Fashion MNIST dataset\n",
    "modified_on_original_scores = model_modified.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on modified, tested on original Fashion MNIST test accuracy: {modified_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the modified model on the modified Fashion MNIST dataset\n",
    "modified_on_modified_scores = model_modified.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on modified, tested on modified Fashion MNIST test accuracy: {modified_on_modified_scores[1] * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot images without labels\n",
    "def plot_images(images, title):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        # Convert single-channel grayscale images to three-channel RGB images for visualization\n",
    "        image = np.squeeze(images[i], axis=-1)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Plot some images from the original Fashion MNIST dataset without labels\n",
    "plot_images(x_train[:10], title='Original Fashion MNIST Images')\n",
    "\n",
    "# Plot some images from the modified Fashion MNIST dataset without labels\n",
    "plot_images(x_train_modified[:10], title='Modified Fashion MNIST Images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1500/1500 [==============================] - 76s 47ms/step - loss: 0.5743 - accuracy: 0.7905 - val_loss: 0.4572 - val_accuracy: 0.8338\n",
      "Epoch 2/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.3699 - accuracy: 0.8654 - val_loss: 0.3865 - val_accuracy: 0.8612\n",
      "Epoch 3/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.3209 - accuracy: 0.8825 - val_loss: 0.3503 - val_accuracy: 0.8705\n",
      "Epoch 4/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.2878 - accuracy: 0.8927 - val_loss: 0.3185 - val_accuracy: 0.8827\n",
      "Epoch 5/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.2625 - accuracy: 0.9030 - val_loss: 0.3090 - val_accuracy: 0.8858\n",
      "Epoch 6/100\n",
      "1500/1500 [==============================] - 63s 42ms/step - loss: 0.2421 - accuracy: 0.9099 - val_loss: 0.2908 - val_accuracy: 0.8947\n",
      "Epoch 7/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.2252 - accuracy: 0.9163 - val_loss: 0.2834 - val_accuracy: 0.8973\n",
      "Epoch 8/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.2117 - accuracy: 0.9210 - val_loss: 0.2902 - val_accuracy: 0.8945\n",
      "Epoch 9/100\n",
      "1500/1500 [==============================] - 65s 44ms/step - loss: 0.1972 - accuracy: 0.9264 - val_loss: 0.3171 - val_accuracy: 0.8827\n",
      "Epoch 10/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.1872 - accuracy: 0.9290 - val_loss: 0.2974 - val_accuracy: 0.8927\n",
      "Epoch 11/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.1752 - accuracy: 0.9343 - val_loss: 0.2936 - val_accuracy: 0.8964\n",
      "Epoch 12/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.1658 - accuracy: 0.9374 - val_loss: 0.2715 - val_accuracy: 0.9055\n",
      "Epoch 13/100\n",
      "1500/1500 [==============================] - 71s 48ms/step - loss: 0.1569 - accuracy: 0.9411 - val_loss: 0.2773 - val_accuracy: 0.8997\n",
      "Epoch 14/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.1483 - accuracy: 0.9439 - val_loss: 0.2799 - val_accuracy: 0.9028\n",
      "Epoch 15/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.1404 - accuracy: 0.9472 - val_loss: 0.2816 - val_accuracy: 0.9013\n",
      "Epoch 16/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.1334 - accuracy: 0.9498 - val_loss: 0.2911 - val_accuracy: 0.9004\n",
      "Epoch 17/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.1242 - accuracy: 0.9526 - val_loss: 0.3060 - val_accuracy: 0.8998\n",
      "Epoch 18/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.1185 - accuracy: 0.9551 - val_loss: 0.3132 - val_accuracy: 0.8977\n",
      "Epoch 19/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.1140 - accuracy: 0.9564 - val_loss: 0.2940 - val_accuracy: 0.9044\n",
      "Epoch 20/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.1071 - accuracy: 0.9593 - val_loss: 0.3115 - val_accuracy: 0.9042\n",
      "Epoch 21/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.1033 - accuracy: 0.9612 - val_loss: 0.3462 - val_accuracy: 0.8959\n",
      "Epoch 22/100\n",
      "1500/1500 [==============================] - 67s 44ms/step - loss: 0.0978 - accuracy: 0.9629 - val_loss: 0.3333 - val_accuracy: 0.8952\n",
      "Epoch 23/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0949 - accuracy: 0.9640 - val_loss: 0.3295 - val_accuracy: 0.8982\n",
      "Epoch 24/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0901 - accuracy: 0.9657 - val_loss: 0.3394 - val_accuracy: 0.9002\n",
      "Epoch 25/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0847 - accuracy: 0.9673 - val_loss: 0.3505 - val_accuracy: 0.8970\n",
      "Epoch 26/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.0834 - accuracy: 0.9681 - val_loss: 0.3490 - val_accuracy: 0.8966\n",
      "Epoch 27/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0770 - accuracy: 0.9713 - val_loss: 0.3658 - val_accuracy: 0.8994\n",
      "Epoch 28/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0775 - accuracy: 0.9707 - val_loss: 0.3976 - val_accuracy: 0.9000\n",
      "Epoch 29/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0730 - accuracy: 0.9724 - val_loss: 0.3882 - val_accuracy: 0.8996\n",
      "Epoch 30/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0721 - accuracy: 0.9731 - val_loss: 0.3822 - val_accuracy: 0.8996\n",
      "Epoch 31/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0719 - accuracy: 0.9725 - val_loss: 0.4187 - val_accuracy: 0.8965\n",
      "Epoch 32/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0654 - accuracy: 0.9748 - val_loss: 0.4093 - val_accuracy: 0.8998\n",
      "Epoch 33/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0632 - accuracy: 0.9761 - val_loss: 0.4131 - val_accuracy: 0.8997\n",
      "Epoch 34/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0632 - accuracy: 0.9765 - val_loss: 0.4121 - val_accuracy: 0.9004\n",
      "Epoch 35/100\n",
      "1500/1500 [==============================] - 68s 45ms/step - loss: 0.0623 - accuracy: 0.9764 - val_loss: 0.4271 - val_accuracy: 0.8980\n",
      "Epoch 36/100\n",
      "1500/1500 [==============================] - 65s 44ms/step - loss: 0.0602 - accuracy: 0.9771 - val_loss: 0.4350 - val_accuracy: 0.8975\n",
      "Epoch 37/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0567 - accuracy: 0.9779 - val_loss: 0.4783 - val_accuracy: 0.9007\n",
      "Epoch 38/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0588 - accuracy: 0.9774 - val_loss: 0.4330 - val_accuracy: 0.8996\n",
      "Epoch 39/100\n",
      "1500/1500 [==============================] - 65s 44ms/step - loss: 0.0519 - accuracy: 0.9802 - val_loss: 0.4533 - val_accuracy: 0.8938\n",
      "Epoch 40/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.0560 - accuracy: 0.9791 - val_loss: 0.4603 - val_accuracy: 0.8972\n",
      "Epoch 41/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0501 - accuracy: 0.9813 - val_loss: 0.4897 - val_accuracy: 0.8883\n",
      "Epoch 42/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0529 - accuracy: 0.9805 - val_loss: 0.4706 - val_accuracy: 0.8960\n",
      "Epoch 43/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0502 - accuracy: 0.9815 - val_loss: 0.4889 - val_accuracy: 0.8957\n",
      "Epoch 44/100\n",
      "1500/1500 [==============================] - 68s 45ms/step - loss: 0.0481 - accuracy: 0.9823 - val_loss: 0.5003 - val_accuracy: 0.8973\n",
      "Epoch 45/100\n",
      "1500/1500 [==============================] - 67s 44ms/step - loss: 0.0522 - accuracy: 0.9807 - val_loss: 0.4779 - val_accuracy: 0.8934\n",
      "Epoch 46/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0467 - accuracy: 0.9828 - val_loss: 0.5421 - val_accuracy: 0.8955\n",
      "Epoch 47/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0447 - accuracy: 0.9836 - val_loss: 0.5377 - val_accuracy: 0.8910\n",
      "Epoch 48/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.5128 - val_accuracy: 0.9011\n",
      "Epoch 49/100\n",
      "1500/1500 [==============================] - 68s 45ms/step - loss: 0.0440 - accuracy: 0.9838 - val_loss: 0.5136 - val_accuracy: 0.8950\n",
      "Epoch 50/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0447 - accuracy: 0.9832 - val_loss: 0.5804 - val_accuracy: 0.8920\n",
      "Epoch 51/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0425 - accuracy: 0.9846 - val_loss: 0.5695 - val_accuracy: 0.8991\n",
      "Epoch 52/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0466 - accuracy: 0.9832 - val_loss: 0.5368 - val_accuracy: 0.8950\n",
      "Epoch 53/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0425 - accuracy: 0.9848 - val_loss: 0.5516 - val_accuracy: 0.8966\n",
      "Epoch 54/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0420 - accuracy: 0.9852 - val_loss: 0.5932 - val_accuracy: 0.8927\n",
      "Epoch 55/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0396 - accuracy: 0.9857 - val_loss: 0.5596 - val_accuracy: 0.8981\n",
      "Epoch 56/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0421 - accuracy: 0.9853 - val_loss: 0.6232 - val_accuracy: 0.8922\n",
      "Epoch 57/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0417 - accuracy: 0.9851 - val_loss: 0.5882 - val_accuracy: 0.8971\n",
      "Epoch 58/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.0425 - accuracy: 0.9848 - val_loss: 0.6043 - val_accuracy: 0.8987\n",
      "Epoch 59/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0373 - accuracy: 0.9865 - val_loss: 0.6340 - val_accuracy: 0.8882\n",
      "Epoch 60/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0409 - accuracy: 0.9853 - val_loss: 0.6030 - val_accuracy: 0.8931\n",
      "Epoch 61/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0384 - accuracy: 0.9862 - val_loss: 0.5860 - val_accuracy: 0.8916\n",
      "Epoch 62/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0372 - accuracy: 0.9868 - val_loss: 0.5869 - val_accuracy: 0.8972\n",
      "Epoch 63/100\n",
      "1500/1500 [==============================] - 70s 47ms/step - loss: 0.0381 - accuracy: 0.9867 - val_loss: 0.6152 - val_accuracy: 0.8951\n",
      "Epoch 64/100\n",
      "1500/1500 [==============================] - 68s 46ms/step - loss: 0.0393 - accuracy: 0.9862 - val_loss: 0.6313 - val_accuracy: 0.8941\n",
      "Epoch 65/100\n",
      "1500/1500 [==============================] - 65s 44ms/step - loss: 0.0346 - accuracy: 0.9875 - val_loss: 0.6307 - val_accuracy: 0.8916\n",
      "Epoch 66/100\n",
      "1500/1500 [==============================] - 64s 43ms/step - loss: 0.0417 - accuracy: 0.9856 - val_loss: 0.6647 - val_accuracy: 0.8972\n",
      "Epoch 67/100\n",
      "1500/1500 [==============================] - 68s 46ms/step - loss: 0.0355 - accuracy: 0.9876 - val_loss: 0.6269 - val_accuracy: 0.8953\n",
      "Epoch 68/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0369 - accuracy: 0.9875 - val_loss: 0.6944 - val_accuracy: 0.8923\n",
      "Epoch 69/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0378 - accuracy: 0.9866 - val_loss: 0.6385 - val_accuracy: 0.8951\n",
      "Epoch 70/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0350 - accuracy: 0.9879 - val_loss: 0.6651 - val_accuracy: 0.8910\n",
      "Epoch 71/100\n",
      "1500/1500 [==============================] - 67s 44ms/step - loss: 0.0352 - accuracy: 0.9879 - val_loss: 0.6631 - val_accuracy: 0.8920\n",
      "Epoch 72/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0328 - accuracy: 0.9886 - val_loss: 0.7210 - val_accuracy: 0.8921\n",
      "Epoch 73/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0347 - accuracy: 0.9882 - val_loss: 0.6890 - val_accuracy: 0.8922\n",
      "Epoch 74/100\n",
      "1500/1500 [==============================] - 66s 44ms/step - loss: 0.0337 - accuracy: 0.9881 - val_loss: 0.6635 - val_accuracy: 0.8942\n",
      "Epoch 75/100\n",
      "1500/1500 [==============================] - 65s 43ms/step - loss: 0.0332 - accuracy: 0.9884 - val_loss: 0.6593 - val_accuracy: 0.8940\n",
      "Epoch 76/100\n",
      "1500/1500 [==============================] - 69s 46ms/step - loss: 0.0330 - accuracy: 0.9891 - val_loss: 0.7135 - val_accuracy: 0.8956\n",
      "Epoch 77/100\n",
      "1500/1500 [==============================] - 67s 45ms/step - loss: 0.0356 - accuracy: 0.9877 - val_loss: 0.7213 - val_accuracy: 0.8948\n",
      "Epoch 78/100\n",
      "1500/1500 [==============================] - 58s 39ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 0.7197 - val_accuracy: 0.8915\n",
      "Epoch 79/100\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.0342 - accuracy: 0.9882 - val_loss: 0.7070 - val_accuracy: 0.8919\n",
      "Epoch 80/100\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.0327 - accuracy: 0.9886 - val_loss: 0.7120 - val_accuracy: 0.8959\n",
      "Epoch 81/100\n",
      "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.7161 - val_accuracy: 0.8933\n",
      "Epoch 82/100\n",
      "1500/1500 [==============================] - 43s 28ms/step - loss: 0.0328 - accuracy: 0.9888 - val_loss: 0.7485 - val_accuracy: 0.8859\n",
      "Epoch 83/100\n",
      "1500/1500 [==============================] - 42s 28ms/step - loss: 0.0314 - accuracy: 0.9894 - val_loss: 0.7300 - val_accuracy: 0.8910\n",
      "Epoch 84/100\n",
      "1500/1500 [==============================] - 41s 27ms/step - loss: 0.0317 - accuracy: 0.9893 - val_loss: 0.7527 - val_accuracy: 0.8972\n",
      "Epoch 85/100\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.0301 - accuracy: 0.9899 - val_loss: 0.7700 - val_accuracy: 0.8933\n",
      "Epoch 86/100\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.0333 - accuracy: 0.9891 - val_loss: 0.7420 - val_accuracy: 0.8978\n",
      "Epoch 87/100\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.0312 - accuracy: 0.9897 - val_loss: 0.7323 - val_accuracy: 0.8961\n",
      "Epoch 88/100\n",
      "1500/1500 [==============================] - 40s 27ms/step - loss: 0.0295 - accuracy: 0.9898 - val_loss: 0.7772 - val_accuracy: 0.8959\n",
      "Epoch 89/100\n",
      "1500/1500 [==============================] - 44s 30ms/step - loss: 0.0321 - accuracy: 0.9895 - val_loss: 0.7865 - val_accuracy: 0.8945\n",
      "Epoch 90/100\n",
      "1500/1500 [==============================] - 43s 29ms/step - loss: 0.0292 - accuracy: 0.9898 - val_loss: 0.7562 - val_accuracy: 0.8975\n",
      "Epoch 91/100\n",
      "1500/1500 [==============================] - 23s 15ms/step - loss: 0.0272 - accuracy: 0.9907 - val_loss: 0.8022 - val_accuracy: 0.8888\n",
      "Epoch 92/100\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0351 - accuracy: 0.9888 - val_loss: 0.7826 - val_accuracy: 0.8928\n",
      "Epoch 93/100\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0288 - accuracy: 0.9903 - val_loss: 0.8197 - val_accuracy: 0.8953\n",
      "Epoch 94/100\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.8035 - val_accuracy: 0.8929\n",
      "Epoch 95/100\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0289 - accuracy: 0.9905 - val_loss: 0.8097 - val_accuracy: 0.8931\n",
      "Epoch 96/100\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0331 - accuracy: 0.9895 - val_loss: 0.7825 - val_accuracy: 0.8926\n",
      "Epoch 97/100\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0270 - accuracy: 0.9907 - val_loss: 0.8338 - val_accuracy: 0.8887\n",
      "Epoch 98/100\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0294 - accuracy: 0.9900 - val_loss: 0.7835 - val_accuracy: 0.8979\n",
      "Epoch 99/100\n",
      "1500/1500 [==============================] - 20s 13ms/step - loss: 0.0290 - accuracy: 0.9910 - val_loss: 0.8006 - val_accuracy: 0.8913\n",
      "Epoch 100/100\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0301 - accuracy: 0.9899 - val_loss: 0.8128 - val_accuracy: 0.8953\n",
      "Model trained on combined, tested on original Fashion MNIST test accuracy: 89.96%\n",
      "Model trained on combined, tested on modified Fashion MNIST test accuracy: 89.07%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the Fashion MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Expand dimensions to add the channel dimension (grayscale images)\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, target_size=(28, 28), scale_factor=0.51):\n",
    "    height, width, channels = image.shape[0], image.shape[1], image.shape[2]\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (target_size[0] - new_height) // 2\n",
    "    pad_width = (target_size[1] - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the target size\n",
    "    if channels == 1:\n",
    "        padded_image = np.zeros((target_size[0], target_size[1]), dtype=np.uint8)\n",
    "    else:\n",
    "        padded_image = np.zeros((target_size[0], target_size[1], channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    if channels == 1:\n",
    "        padded_image = np.expand_dims(padded_image, axis=-1)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Concatenate the original and modified data\n",
    "x_train_combined = np.concatenate((x_train, x_train_modified), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_train), axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10  # Fashion MNIST has 10 classes\n",
    "y_train_combined = tf.keras.utils.to_categorical(y_train_combined, num_classes=num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Define the model architecture\n",
    "input_shape = (28, 28, 1)  # Input shape for Fashion MNIST images\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "model.fit(x_train_combined, y_train_combined, epochs=100, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original Fashion MNIST dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on original Fashion MNIST test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified Fashion MNIST dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on modified Fashion MNIST test accuracy: {original_on_modified_scores[1] * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
