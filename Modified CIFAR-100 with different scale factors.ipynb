{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\d4\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\d4\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\d4\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\d4\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\d4\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1250/1250 [==============================] - 74s 40ms/step - loss: 3.9980 - accuracy: 0.0857 - val_loss: 3.6034 - val_accuracy: 0.1437\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 3.3799 - accuracy: 0.1880 - val_loss: 3.2589 - val_accuracy: 0.2113\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 3.0864 - accuracy: 0.2405 - val_loss: 3.0819 - val_accuracy: 0.2435\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.8870 - accuracy: 0.2805 - val_loss: 2.9339 - val_accuracy: 0.2705\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 2.7323 - accuracy: 0.3099 - val_loss: 2.8468 - val_accuracy: 0.2885\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 2.6177 - accuracy: 0.3312 - val_loss: 2.7823 - val_accuracy: 0.3006\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 2.5330 - accuracy: 0.3494 - val_loss: 2.7467 - val_accuracy: 0.3110\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 2.4663 - accuracy: 0.3634 - val_loss: 2.6952 - val_accuracy: 0.3228\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 2.3964 - accuracy: 0.3800 - val_loss: 2.6893 - val_accuracy: 0.3191\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 29s 24ms/step - loss: 2.3483 - accuracy: 0.3879 - val_loss: 2.7082 - val_accuracy: 0.3238\n",
      "Model trained on combined, tested on original CIFAR-100 test accuracy: 34.73%\n",
      "Model trained on combined, tested on modified CIFAR-100 test accuracy: 31.75%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, scale_factor=0.5):\n",
    "    height, width, channels = image.shape\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (height - new_height) // 2\n",
    "    pad_width = (width - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the same size as the original image\n",
    "    padded_image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Concatenate the original and modified data\n",
    "x_train_combined = np.concatenate((x_train, x_train_modified), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_train), axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_combined = tf.keras.utils.to_categorical(y_train_combined, num_classes=100)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "model.fit(x_train_combined, y_train_combined, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original CIFAR-100 dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on original CIFAR-100 test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified CIFAR-100 dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on modified CIFAR-100 test accuracy: {original_on_modified_scores[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 38s 23ms/step - loss: 3.9208 - accuracy: 0.1003 - val_loss: 3.4846 - val_accuracy: 0.1651\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 3.2794 - accuracy: 0.2034 - val_loss: 3.2143 - val_accuracy: 0.2150\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 2.9788 - accuracy: 0.2620 - val_loss: 2.9757 - val_accuracy: 0.2624\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 2.8028 - accuracy: 0.2957 - val_loss: 2.8233 - val_accuracy: 0.2890\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 28s 22ms/step - loss: 2.6641 - accuracy: 0.3217 - val_loss: 2.7550 - val_accuracy: 0.3063\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 2.5618 - accuracy: 0.3419 - val_loss: 2.6656 - val_accuracy: 0.3198\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 2.4739 - accuracy: 0.3622 - val_loss: 2.6844 - val_accuracy: 0.3247\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 2.4041 - accuracy: 0.3768 - val_loss: 2.5490 - val_accuracy: 0.3489\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 21s 17ms/step - loss: 2.3417 - accuracy: 0.3913 - val_loss: 2.5615 - val_accuracy: 0.3470\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 2.2900 - accuracy: 0.3998 - val_loss: 2.5605 - val_accuracy: 0.3490\n",
      "Model trained on combined, tested on original CIFAR-100 test accuracy: 36.21%\n",
      "Model trained on combined, tested on modified CIFAR-100 test accuracy: 33.04%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, scale_factor=0.6):\n",
    "    height, width, channels = image.shape\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (height - new_height) // 2\n",
    "    pad_width = (width - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the same size as the original image\n",
    "    padded_image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Concatenate the original and modified data\n",
    "x_train_combined = np.concatenate((x_train, x_train_modified), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_train), axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_combined = tf.keras.utils.to_categorical(y_train_combined, num_classes=100)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "model.fit(x_train_combined, y_train_combined, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original CIFAR-100 dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on original CIFAR-100 test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified CIFAR-100 dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on modified CIFAR-100 test accuracy: {original_on_modified_scores[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 277s 160ms/step - loss: 3.9207 - accuracy: 0.0998 - val_loss: 3.4520 - val_accuracy: 0.1773\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 3.2593 - accuracy: 0.2103 - val_loss: 3.0968 - val_accuracy: 0.2412\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 2.9766 - accuracy: 0.2632 - val_loss: 2.8852 - val_accuracy: 0.2862\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 2.7952 - accuracy: 0.2959 - val_loss: 2.8186 - val_accuracy: 0.2970\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 2.6599 - accuracy: 0.3230 - val_loss: 2.7024 - val_accuracy: 0.3181\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 2.5546 - accuracy: 0.3457 - val_loss: 2.6510 - val_accuracy: 0.3289\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 2.4694 - accuracy: 0.3633 - val_loss: 2.5756 - val_accuracy: 0.3447\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 26s 21ms/step - loss: 2.3991 - accuracy: 0.3780 - val_loss: 2.5857 - val_accuracy: 0.3469\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 2.3332 - accuracy: 0.3902 - val_loss: 2.4998 - val_accuracy: 0.3579\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 2.2776 - accuracy: 0.4044 - val_loss: 2.4794 - val_accuracy: 0.3659\n",
      "Model trained on combined, tested on original CIFAR-100 test accuracy: 35.58%\n",
      "Model trained on combined, tested on modified CIFAR-100 test accuracy: 34.84%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, scale_factor=0.7):\n",
    "    height, width, channels = image.shape\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (height - new_height) // 2\n",
    "    pad_width = (width - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the same size as the original image\n",
    "    padded_image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Concatenate the original and modified data\n",
    "x_train_combined = np.concatenate((x_train, x_train_modified), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_train), axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_combined = tf.keras.utils.to_categorical(y_train_combined, num_classes=100)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "model.fit(x_train_combined, y_train_combined, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original CIFAR-100 dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on original CIFAR-100 test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified CIFAR-100 dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on modified CIFAR-100 test accuracy: {original_on_modified_scores[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 46s 26ms/step - loss: 3.8830 - accuracy: 0.1057 - val_loss: 3.4450 - val_accuracy: 0.1822\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 29s 23ms/step - loss: 3.2249 - accuracy: 0.2190 - val_loss: 3.0275 - val_accuracy: 0.2533\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 2.9092 - accuracy: 0.2773 - val_loss: 2.8655 - val_accuracy: 0.2869\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 2.6977 - accuracy: 0.3158 - val_loss: 2.6383 - val_accuracy: 0.3358\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 2.5421 - accuracy: 0.3486 - val_loss: 2.5273 - val_accuracy: 0.3561\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.4285 - accuracy: 0.3724 - val_loss: 2.4810 - val_accuracy: 0.3668\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.3409 - accuracy: 0.3919 - val_loss: 2.4577 - val_accuracy: 0.3692\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 42s 33ms/step - loss: 2.2722 - accuracy: 0.4053 - val_loss: 2.4999 - val_accuracy: 0.3629\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.2103 - accuracy: 0.4172 - val_loss: 2.3816 - val_accuracy: 0.3842\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 46s 37ms/step - loss: 2.1535 - accuracy: 0.4327 - val_loss: 2.3535 - val_accuracy: 0.3901\n",
      "Model trained on combined, tested on original CIFAR-100 test accuracy: 36.58%\n",
      "Model trained on combined, tested on modified CIFAR-100 test accuracy: 36.52%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, scale_factor=0.8):\n",
    "    height, width, channels = image.shape\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (height - new_height) // 2\n",
    "    pad_width = (width - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the same size as the original image\n",
    "    padded_image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Concatenate the original and modified data\n",
    "x_train_combined = np.concatenate((x_train, x_train_modified), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_train), axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_combined = tf.keras.utils.to_categorical(y_train_combined, num_classes=100)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "model.fit(x_train_combined, y_train_combined, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original CIFAR-100 dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on original CIFAR-100 test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified CIFAR-100 dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on modified CIFAR-100 test accuracy: {original_on_modified_scores[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 81s 42ms/step - loss: 3.8155 - accuracy: 0.1204 - val_loss: 3.3310 - val_accuracy: 0.1997\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 47s 37ms/step - loss: 3.1431 - accuracy: 0.2311 - val_loss: 2.9550 - val_accuracy: 0.2740\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.8447 - accuracy: 0.2901 - val_loss: 2.7709 - val_accuracy: 0.3120\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.6504 - accuracy: 0.3285 - val_loss: 2.6660 - val_accuracy: 0.3245\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 46s 36ms/step - loss: 2.5110 - accuracy: 0.3577 - val_loss: 2.5085 - val_accuracy: 0.3611\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.4057 - accuracy: 0.3796 - val_loss: 2.4378 - val_accuracy: 0.3766\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 45s 36ms/step - loss: 2.3170 - accuracy: 0.3979 - val_loss: 2.4211 - val_accuracy: 0.3775\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 47s 38ms/step - loss: 2.2453 - accuracy: 0.4126 - val_loss: 2.3943 - val_accuracy: 0.3864\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 44s 35ms/step - loss: 2.1846 - accuracy: 0.4261 - val_loss: 2.3487 - val_accuracy: 0.3947\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 43s 35ms/step - loss: 2.1270 - accuracy: 0.4387 - val_loss: 2.3255 - val_accuracy: 0.3981\n",
      "Model trained on combined, tested on original CIFAR-100 test accuracy: 36.13%\n",
      "Model trained on combined, tested on modified CIFAR-100 test accuracy: 35.95%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.datasets import cifar100\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "# Modify the training and test data\n",
    "def resize_and_pad(image, scale_factor=0.9):\n",
    "    height, width, channels = image.shape\n",
    "    new_height = int(height * scale_factor)\n",
    "    new_width = int(width * scale_factor)\n",
    "\n",
    "    # Resize the image\n",
    "    resized_image = cv2.resize(image, (new_width, new_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # Calculate the padding required to maintain the original aspect ratio\n",
    "    pad_height = (height - new_height) // 2\n",
    "    pad_width = (width - new_width) // 2\n",
    "\n",
    "    # Create a new padded image with the same size as the original image\n",
    "    padded_image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "    # Paste the resized image onto the padded image\n",
    "    padded_image[pad_height:pad_height+new_height, pad_width:pad_width+new_width] = resized_image\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Modify the training and test data\n",
    "x_train_modified = np.array([resize_and_pad(image) for image in x_train])\n",
    "x_test_modified = np.array([resize_and_pad(image) for image in x_test])\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "x_train_modified = x_train_modified / 255.0\n",
    "x_test_modified = x_test_modified / 255.0\n",
    "\n",
    "# Concatenate the original and modified data\n",
    "x_train_combined = np.concatenate((x_train, x_train_modified), axis=0)\n",
    "y_train_combined = np.concatenate((y_train, y_train), axis=0)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train_combined = tf.keras.utils.to_categorical(y_train_combined, num_classes=100)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=100)\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(100, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "model.fit(x_train_combined, y_train_combined, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the original CIFAR-100 dataset\n",
    "original_on_original_scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on original CIFAR-100 test accuracy: {original_on_original_scores[1] * 100:.2f}%')\n",
    "\n",
    "# Evaluate the model on the modified CIFAR-100 dataset\n",
    "original_on_modified_scores = model.evaluate(x_test_modified, y_test, verbose=0)\n",
    "print(f'Model trained on combined, tested on modified CIFAR-100 test accuracy: {original_on_modified_scores[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
